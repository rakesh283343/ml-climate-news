# Purpose

ML Climate News is an end-to-end machine learning platform [serving climate change fake news](https://serve-climate-news-dot-climate-poc-01.appspot.com/).
Sadly, this proves that it is easier than ever to spread disinformation!

We want to raise awareness about this, and link to more credible sources.

To learn about the real science of Climate Change:

[Royal Society: Climate Change Basics](https://royalsociety.org/topics-policy/projects/climate-change-evidence-causes/basics-of-climate-change/)  
[NASA: Evidence of Global Climate Change](https://climate.nasa.gov/evidence/)  
[IPCC: Climate Change report 2013](https://www.ipcc.ch/report/ar5/wg1/)  
[Science Net Links: The Science of Climate Change](http://sciencenetlinks.com/collections/climate-change/)  

It also doubles up as a *multi-purpose machine learning platform* running on GCP. It can ingest any kind of data, store it, train models, and serve them as apps.

# Prerequisites

## Tooling

* terraform 0.12
* gcloud
* gsutil

## Set up

* gcp account with billing enabled
* gcloud login
* `terraform.tfvars` file with settings based on `terraform.tfvars.example` template

# Install

```
git clone https://github.com/thundercomb/gcp-climate-analytics
cd gcp-climate-analytics
bash start.sh
```

This process takes several minutes. You should eventually see the following messages:

```
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
```

This means the Kubeflow frontend is now available. Navigate to `http://localhost:8080`.

# How it works

The Climate News ML platform is a fully fledged end-to-end machine learning platform.
The simplest way to think of the process is as follows:

*Ingestion -> Data -> Machine Learning -> Models -> Serving*

## Ingestion

Machine Learning models rely on data. The Google App Engine ingestion services first ingest data and then insert the data into BigQuery tables.

There are currently two ingestion services: ncei and clnn. As the platform matures, more data sources will be added.

Cloud Scheduler ingest the latest CLNN news on a schedule.

The ingestion can also be run as a one-off as follows.

Navigate to

`https://ncei-wind-dot-<my-project-id>.appspot.com/`

The web page should receive 'ok'

Now ingest data by sending a request with

`https://ncei-wind-dot-<my-project-id>.appspot.com/ingest`

The example data is now in a BigQuery table called `wind` in dataset `ncei`

To see all of the available ingestion services, navigate to App Engine on the console:

`https://console.cloud.google.com/appengine/services?project=<your-project-id>`

Or use gcloud:

`gcloud app services list --project=<your-project-id>`

All services starting `ingest-` are ingestion services.

## Data

By default ingestion data is stored in BigQuery. Sources are logically separated by datasets rather than by table, to permit fine-grained control over ACLs.

## Machine Learning

Machine Learning requires a platform on which to run experiments and save models that make the cut. Kubeflow provides a number of capabilities through Katib experiments, Argo workflows, and Kubeflow pipelines.

We use a simple Argo workflow, which can be uploaded via the Kubeflow dashboard, to train a model and save it as a versioned artefact on GCS.

### Iris Demo

The iris dataset is used as an example to test the workflow. Upload the iris ```workflow.yaml``` file (in the ```ml-train-iris``` directory), create a Kubeflow Run, and look for the model output in the configured storage bucket.

### Climate News

The climate news training process uses OpenAI's [GPT-2](https://openai.com/blog/better-language-models/) language model and Minimaxir's [gpt_2_simple](https://github.com/minimaxir/gpt-2-simple) library to finetune on data from news sources such as the Climate News Network. It has its own ```workflow.yaml``` in the ```ml-train-climate-news``` directory.

## Models

The models are stored in directories on GCS. Kubeflow provides [Minio](https://min.io/) out of the box, which is another option. Minio requires the cluster to be running, so for most purposes GCS is both cheaper and more available.

### Inference and Prediction

For larger models that take more time during inferencing a separate process can speed up serving.
The ```ml-generate-climate-news``` pipeline is configured to run daily using Cloud Scheduler so that a new article is generated daily.

## Serving

The serving web services can use the trained models directly, or, when prediction and inferencing is time consuming, serve outputs generated by an inferencing process. The serving web services run on Google App Engine.

The iris sample web service is a good entrypoint. Once you have trained a model using Kubeflow, navigate to the home page of the iris serving app:

`https://serve-iris-predictions-dot-<my-project-id>.appspot.com/`

You will be shown the iris model's targets and predictions.

The Climate News serving app is available at:

`https://serve-iris-predictions-dot-<my-project-id>.appspot.com/`

# DevOps and MLOps

The platform provides the building blocks to manage code and ml model artefacts according to DevOps and MLOps principles.

 * Cloud Build CI pipelines build ML images and push them to Google's Container Registry (gcr.io).
 * Cloud Build CD pipelines deploy code repos for ingestion and serving services to Google App Engine.
 * Kubeflow provides choice in the form of Kubeflow Pipelines, Argo, and Katib in terms of how to experiment and train models.

There is much, much more to DevOps and MLOps. More about that another time...

# Troubleshooting

## Failed start script

## Failed App Engine service

If a service does not come up, go to the App Engine dashboard and look for the relevant service. Look at the logs for clues as to why it might be failing.
